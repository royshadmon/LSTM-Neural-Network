{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/karthik/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf2 = pd.read_table(\\'glove.twitter.27B.25d.txt\\', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\\n\\ndef vectorize(w):\\n  return df2.loc[w].as_matrix()\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk \n",
    "\n",
    "import gensim as gs\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df1 = pd.read_csv('HillaryTrumpTrainWithTags.csv')\n",
    "df2 = pd.read_csv('TrumpHillaryTest.csv')\n",
    "\"\"\"\n",
    "df2 = pd.read_table('glove.twitter.27B.25d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def vectorize(w):\n",
    "  return df2.loc[w].as_matrix()\n",
    "\"\"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tokenized_tweets'] = df1.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "df2['tokenized_tweets'] = df2.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df1['tokenized_tweets']\n",
    "tweets_test = df2['tokenized_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "_ = df1.apply(lambda row: length.append(len(row['tokenized_tweets'])), axis=1)\n",
    "max_length = max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = gs.models.Word2Vec(df1['tokenized_tweets'].append(df2['tokenized_tweets']) , min_count=1, size = 25, iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    #print(type(string))\n",
    "    train_matrix.append(vectorizer.wv[tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(train_matrix).reshape(len(train_matrix),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 25)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix = np.array(train_matrix)\n",
    "train_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = train_matrix[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(twits):\n",
    "    \"\"\"\n",
    "    Does the padding as many times as required\n",
    "    \"\"\"\n",
    "    l = len(twits)\n",
    "    d= twits.shape[1]\n",
    "    padding = np.zeros(d)\n",
    "    \n",
    "    for i in range(0,max_length-l):\n",
    "        twits = np.append(twits,padding)\n",
    "        \n",
    "    return twits.reshape(max_length,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_matrix(t_matrix):\n",
    "    \"\"\"\n",
    "    Loops through all the vectorized tweets and calls the pad function on each of them.\n",
    "    \"\"\"\n",
    "    \n",
    "    padded_matrix =[]\n",
    "    for i,vectorized_tweet in enumerate(t_matrix):\n",
    "         padded_matrix.append(pad(vectorized_tweet))\n",
    "    padded_matrix = np.array(padded_matrix)\n",
    "    \n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_matrix = pad_matrix(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 54, 25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 54, 25)\n"
     ]
    }
   ],
   "source": [
    "data_tf = tf.convert_to_tensor(padded_train_matrix, np.float32)\n",
    "\n",
    "sess = tf.InteractiveSession()  \n",
    "print(data_tf.eval().shape)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
