{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Alex/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf2 = pd.read_table(\\'glove.twitter.27B.25d.txt\\', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\\n\\ndef vectorize(w):\\n  return df2.loc[w].as_matrix()\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk \n",
    "\n",
    "import gensim as gs\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df1 = pd.read_csv('HillaryTrumpTrainWithTags.csv')\n",
    "df2 = pd.read_csv('TrumpHillaryTest.csv')\n",
    "\"\"\"\n",
    "df2 = pd.read_table('glove.twitter.27B.25d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def vectorize(w):\n",
    "  return df2.loc[w].as_matrix()\n",
    "\"\"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleToNumber(handle):\n",
    "    if handle == \"realDonaldTrump\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tokenized_tweets'] = df1.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "df1['labels'] = df1.apply(lambda row: handleToNumber(row['handle']), axis=1)\n",
    "df2['tokenized_tweets'] = df2.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df1['tokenized_tweets']\n",
    "tweets_test = df2['tokenized_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "_ = df1.apply(lambda row: length.append(len(row['tokenized_tweets'])), axis=1)\n",
    "max_length = max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = gs.models.Word2Vec(df1['tokenized_tweets'].append(df2['tokenized_tweets']) , min_count=1, size = 25, iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    #print(type(string))\n",
    "    train_matrix.append(vectorizer.wv[tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(train_matrix).reshape(len(train_matrix),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.39693737e-02, -2.80195862e-01,  1.64104879e+00,\n",
       "         1.41863525e-01,  6.24521136e-01, -3.58341157e-01,\n",
       "        -1.92452103e-01, -5.31973243e-01,  1.59815454e+00,\n",
       "         9.93977726e-01,  2.09545031e-01, -9.64298099e-02,\n",
       "         5.62113672e-02, -7.47769058e-01, -3.99524659e-01,\n",
       "        -1.18690169e+00, -2.25959134e+00,  3.96619976e-01,\n",
       "         3.51921469e-01, -1.05039632e+00,  1.26533878e+00,\n",
       "         6.22419238e-01,  6.46917164e-01,  5.51957130e-01,\n",
       "         2.45440125e-01],\n",
       "       [-6.23479113e-02,  1.53254986e-01,  3.61090690e-01,\n",
       "         2.39134073e-01,  3.29183251e-01, -3.03277016e-01,\n",
       "         2.58549750e-01,  1.09864622e-01,  3.97847295e-01,\n",
       "         6.39620185e-01,  1.21509470e-01,  9.76631120e-02,\n",
       "        -1.68215886e-01, -3.64276797e-01,  1.65536717e-01,\n",
       "        -4.96967822e-01, -5.93378663e-01, -1.00739367e-01,\n",
       "         1.30573109e-01, -2.38590568e-01,  2.05842763e-01,\n",
       "         2.17453137e-01,  4.52822834e-01,  2.17257068e-01,\n",
       "         2.55629629e-01],\n",
       "       [ 9.98444140e-01, -2.07706213e+00,  1.29821348e+00,\n",
       "         2.67800570e-01,  7.87577510e-01, -8.35095644e-01,\n",
       "        -5.61300695e-01, -1.31522727e+00,  2.57529712e+00,\n",
       "         5.70498824e-01,  2.11575437e+00, -2.58703738e-01,\n",
       "        -1.53327569e-01, -1.92169952e+00, -4.76806253e-01,\n",
       "        -5.53758144e-01, -2.11976051e+00,  4.04639244e-01,\n",
       "         1.12794650e+00, -1.30717635e+00,  7.77308270e-02,\n",
       "         1.86785793e+00,  1.25283551e+00,  2.11832690e+00,\n",
       "         4.19847667e-01],\n",
       "       [-1.47821615e-02, -1.08625069e-02,  2.52919078e-01,\n",
       "         7.57786036e-02,  1.95345551e-01, -1.40120879e-01,\n",
       "         7.70077482e-02,  6.74574496e-03,  2.63841450e-01,\n",
       "         2.70961344e-01,  1.14220321e-01,  6.76817447e-02,\n",
       "        -1.82970259e-02, -1.86877742e-01,  7.56351203e-02,\n",
       "        -2.61206508e-01, -3.11672628e-01, -1.50777129e-02,\n",
       "         7.31671751e-02, -5.82770593e-02,  9.12548453e-02,\n",
       "         1.56642556e-01,  2.51468837e-01,  1.30192935e-01,\n",
       "         1.58295393e-01],\n",
       "       [-4.20259148e-01, -3.19867015e-01,  8.67313683e-01,\n",
       "        -1.97544217e-01,  7.66809940e-01,  8.55634455e-03,\n",
       "         6.11322165e-01,  9.27285910e-01,  1.21211159e+00,\n",
       "         9.22760606e-01,  8.82544816e-01,  3.00978720e-01,\n",
       "         3.08166016e-02, -2.90502697e-01,  5.45000613e-01,\n",
       "        -6.72466755e-01, -1.13797677e+00, -6.98215425e-01,\n",
       "         9.01776433e-01, -4.77782547e-01, -3.62209231e-01,\n",
       "         1.00941680e-01,  7.11170137e-01,  6.82035625e-01,\n",
       "         6.88780367e-01],\n",
       "       [-8.99166703e-01, -2.94712496e+00,  3.67736077e+00,\n",
       "         5.80376208e-01, -3.34338881e-02,  2.48336363e+00,\n",
       "        -1.76322043e+00,  3.67637128e-01,  3.06823587e+00,\n",
       "         4.65845394e+00,  1.50914049e+00, -2.28490806e+00,\n",
       "         8.11191976e-01, -1.15736032e+00,  2.64280081e+00,\n",
       "        -2.66565466e+00, -6.89485013e-01,  7.64631569e-01,\n",
       "         1.88709724e+00, -7.43217766e-01, -2.66718030e-01,\n",
       "         6.89372897e-01,  2.08589125e+00,  2.63286054e-01,\n",
       "         1.93319941e+00],\n",
       "       [-1.35982502e+00, -3.94617462e+00,  1.78285587e+00,\n",
       "         1.31558943e+00, -5.80260515e-01, -6.24825716e-01,\n",
       "        -9.05942738e-01, -2.27721334e+00,  3.31421852e+00,\n",
       "         5.92264831e-01,  1.78376392e-01, -7.62041807e-01,\n",
       "         5.00241876e-01, -4.37033951e-01,  5.72772563e-01,\n",
       "        -4.57051009e-01, -2.81729722e+00,  4.87026840e-01,\n",
       "         9.33447242e-01, -1.37000883e+00,  1.67629850e+00,\n",
       "         2.52330512e-01,  3.56139779e+00,  4.48052466e-01,\n",
       "        -2.11850494e-01],\n",
       "       [-2.34182790e-01, -2.87148547e+00,  1.84611487e+00,\n",
       "         1.66460347e+00,  1.30976760e+00, -5.15017629e-01,\n",
       "        -3.61772537e-01, -1.77738535e+00,  3.39600348e+00,\n",
       "         7.91212380e-01,  2.41169786e+00, -4.98583555e-01,\n",
       "         2.28817844e+00, -3.25694084e-01,  8.95752683e-02,\n",
       "        -1.19347942e+00, -2.55358958e+00,  8.20805848e-01,\n",
       "         1.03883827e+00,  1.12981856e+00,  1.25515774e-01,\n",
       "         2.55162150e-01,  3.02649331e+00,  1.91989720e+00,\n",
       "         7.30560660e-01],\n",
       "       [-1.09446064e-01, -1.97217858e+00,  8.68166327e-01,\n",
       "         2.04921627e+00,  1.29273331e+00, -3.12331724e+00,\n",
       "         6.34168506e-01, -2.59166193e+00,  2.05034924e+00,\n",
       "        -4.73887384e-01,  1.30332446e+00, -3.47387344e-02,\n",
       "        -1.43035412e-01, -7.66657889e-01, -1.31093395e+00,\n",
       "        -1.19889230e-01, -3.30999565e+00, -1.49102852e-01,\n",
       "         1.11104965e-01,  4.02223952e-02,  6.36626661e-01,\n",
       "         1.64873111e+00,  3.30024171e+00,  1.21930134e+00,\n",
       "        -3.41087908e-01],\n",
       "       [-1.61333561e+00, -2.93575931e+00,  1.49956715e+00,\n",
       "         3.16925716e+00,  9.72884297e-01, -2.26947498e+00,\n",
       "        -4.40744519e-01, -3.27979159e+00,  2.29054546e+00,\n",
       "         5.66215754e-01,  1.75312161e+00,  5.11775196e-01,\n",
       "         2.77966094e+00, -5.79792082e-01,  9.15509343e-01,\n",
       "        -2.45994377e+00, -1.48340380e+00,  9.61640418e-01,\n",
       "         1.29601285e-01,  5.51657975e-01,  2.06401157e+00,\n",
       "         9.07918692e-01,  2.66028190e+00,  2.22342753e+00,\n",
       "         3.25480849e-02],\n",
       "       [-1.22533396e-01, -2.10253313e-01,  2.84289122e-02,\n",
       "         3.44120450e-02, -5.25300801e-02, -4.29223366e-02,\n",
       "        -1.86288357e-01, -1.47432089e-01,  1.14386983e-01,\n",
       "         3.26496433e-03,  1.55068263e-01, -2.88017131e-02,\n",
       "         1.92758530e-01, -6.21478073e-02,  1.00423403e-01,\n",
       "         3.52909206e-03, -4.62807417e-02,  1.19461507e-01,\n",
       "        -1.23018697e-01, -3.91327702e-02, -6.11530095e-02,\n",
       "        -7.13700280e-02,  2.46771611e-02, -1.82159021e-02,\n",
       "        -1.51469946e-01],\n",
       "       [-9.89498720e-02, -1.49326265e-01,  2.01662958e-01,\n",
       "         2.00614035e-01,  1.29508495e-01, -1.67096600e-01,\n",
       "         1.21026367e-01, -3.84924263e-02,  1.93558961e-01,\n",
       "         1.48955762e-01,  2.13194132e-01,  4.99351360e-02,\n",
       "         5.81175610e-02, -1.93774521e-01,  9.71274152e-02,\n",
       "        -1.30994618e-01, -1.55288324e-01, -2.93803047e-02,\n",
       "         1.84595495e-01, -3.38245556e-02,  6.84156269e-02,\n",
       "         9.35497507e-02,  3.47173572e-01,  2.03580499e-01,\n",
       "         1.23025820e-01],\n",
       "       [ 4.89566401e-02, -4.07123327e+00,  2.91776705e+00,\n",
       "         1.45451784e+00,  1.08051240e+00, -1.24732852e+00,\n",
       "        -2.48778844e+00, -3.73441505e+00,  3.48574519e+00,\n",
       "         1.74385026e-01,  1.92778325e+00, -1.06018388e+00,\n",
       "         2.25288415e+00, -8.80699217e-01,  5.07104036e-04,\n",
       "        -6.81478262e-01, -3.57531047e+00,  1.64140296e+00,\n",
       "        -2.82230258e-01, -8.79158735e-01,  1.35820043e+00,\n",
       "         1.37542820e+00,  2.48564053e+00,  9.34874237e-01,\n",
       "        -5.52519441e-01],\n",
       "       [-1.43878174e+00, -2.79435849e+00,  1.46440208e+00,\n",
       "        -2.07181871e-01,  7.27043629e-01, -7.05986738e-01,\n",
       "        -9.94054973e-01, -1.37744904e-01,  3.54959440e+00,\n",
       "         8.45975876e-01,  1.63573158e+00, -8.87636244e-02,\n",
       "         1.78463614e+00, -5.91585815e-01,  5.73769271e-01,\n",
       "        -7.03788757e-01, -2.78236985e+00,  1.55961919e+00,\n",
       "         9.39252198e-01, -8.21182430e-01,  5.42649150e-01,\n",
       "        -3.58271211e-01,  9.86941755e-01,  9.18386698e-01,\n",
       "        -1.00267015e-01],\n",
       "       [-1.35982502e+00, -3.94617462e+00,  1.78285587e+00,\n",
       "         1.31558943e+00, -5.80260515e-01, -6.24825716e-01,\n",
       "        -9.05942738e-01, -2.27721334e+00,  3.31421852e+00,\n",
       "         5.92264831e-01,  1.78376392e-01, -7.62041807e-01,\n",
       "         5.00241876e-01, -4.37033951e-01,  5.72772563e-01,\n",
       "        -4.57051009e-01, -2.81729722e+00,  4.87026840e-01,\n",
       "         9.33447242e-01, -1.37000883e+00,  1.67629850e+00,\n",
       "         2.52330512e-01,  3.56139779e+00,  4.48052466e-01,\n",
       "        -2.11850494e-01],\n",
       "       [-3.71438694e+00,  1.15312493e+00,  1.10647285e+00,\n",
       "         9.16150749e-01,  2.08416462e+00, -6.67563140e-01,\n",
       "         2.77924466e+00,  1.19571698e+00,  4.50649321e-01,\n",
       "         3.11231923e+00, -1.65598190e+00,  1.27705550e+00,\n",
       "         3.26316416e-01,  3.25069577e-02,  1.57810569e+00,\n",
       "        -2.62184334e+00, -2.88624787e+00, -1.52697802e+00,\n",
       "         1.10151958e+00, -7.14094162e-01,  1.23575401e+00,\n",
       "         1.46428859e+00,  8.75102282e-01, -7.64004290e-01,\n",
       "         9.36276138e-01],\n",
       "       [-1.76730776e+00,  2.81739950e+00,  2.40278077e+00,\n",
       "         5.72117090e-01,  7.69274294e-01, -1.88361120e+00,\n",
       "         3.13156033e+00,  2.53685927e+00,  1.82478392e+00,\n",
       "         3.69555449e+00, -7.03998730e-02, -5.89575708e-01,\n",
       "        -1.02528465e+00, -7.89410993e-02,  1.85355854e+00,\n",
       "        -2.51423407e+00, -1.72645330e+00, -2.62016463e+00,\n",
       "        -3.04397225e-01, -3.07345819e+00,  3.30085337e-01,\n",
       "         9.04740930e-01,  3.18669653e+00,  7.94089735e-01,\n",
       "         3.19119406e+00],\n",
       "       [-1.44701648e+00,  1.51503551e+00,  8.75564873e-01,\n",
       "         1.58242500e+00,  1.25664043e+00, -8.12931716e-01,\n",
       "         4.59726620e+00,  5.24586737e-01,  1.60854781e+00,\n",
       "         3.69168472e+00, -8.26111257e-01,  1.15606415e+00,\n",
       "         1.06309843e+00, -1.01428048e-03,  1.12436450e+00,\n",
       "        -4.30764389e+00, -1.88352096e+00, -1.86531591e+00,\n",
       "        -2.72221658e-02, -1.72358632e+00,  1.24061882e+00,\n",
       "        -9.94410038e-01,  4.59891170e-01,  7.08988547e-01,\n",
       "         2.49599385e+00],\n",
       "       [-2.17347360e+00,  1.20951223e+00,  2.20994925e+00,\n",
       "         1.05454922e+00,  1.50792325e+00, -1.87066054e+00,\n",
       "         3.34557772e+00,  1.75407577e+00,  1.80561137e+00,\n",
       "         3.15705371e+00, -8.45008064e-03,  2.17608690e-01,\n",
       "        -2.49674067e-01, -7.71011770e-01,  1.95731032e+00,\n",
       "        -2.35102391e+00, -1.69964540e+00, -2.15888643e+00,\n",
       "         1.32653379e+00, -2.50456953e+00,  6.10198796e-01,\n",
       "         8.42055321e-01,  2.70428395e+00,  5.42826533e-01,\n",
       "         2.78003860e+00],\n",
       "       [-9.95563030e-01, -4.15334046e-01,  8.35551560e-01,\n",
       "        -1.98635250e-01,  1.10240316e+00, -1.53785813e+00,\n",
       "         1.45851040e+00,  1.14317250e+00,  2.32448339e+00,\n",
       "         2.24091792e+00,  1.24948025e+00, -4.32111442e-01,\n",
       "        -1.40071380e+00, -1.46986353e+00,  2.05575585e+00,\n",
       "        -1.00448775e+00, -9.83881831e-01, -1.04047775e+00,\n",
       "         1.44079888e+00, -2.20346284e+00, -9.52077091e-01,\n",
       "         4.70954180e-01,  2.76472688e+00,  1.24586737e+00,\n",
       "         2.35069084e+00],\n",
       "       [-3.71438694e+00,  1.15312493e+00,  1.10647285e+00,\n",
       "         9.16150749e-01,  2.08416462e+00, -6.67563140e-01,\n",
       "         2.77924466e+00,  1.19571698e+00,  4.50649321e-01,\n",
       "         3.11231923e+00, -1.65598190e+00,  1.27705550e+00,\n",
       "         3.26316416e-01,  3.25069577e-02,  1.57810569e+00,\n",
       "        -2.62184334e+00, -2.88624787e+00, -1.52697802e+00,\n",
       "         1.10151958e+00, -7.14094162e-01,  1.23575401e+00,\n",
       "         1.46428859e+00,  8.75102282e-01, -7.64004290e-01,\n",
       "         9.36276138e-01],\n",
       "       [-1.97595382e+00,  1.92970014e+00,  2.75702333e+00,\n",
       "         1.39091229e+00, -1.17412001e-01, -1.21008158e+00,\n",
       "         2.06216502e+00,  1.17562723e+00,  2.21151090e+00,\n",
       "         2.84186745e+00, -1.65603364e+00, -7.84672379e-01,\n",
       "         9.18561637e-01,  2.23212671e+00,  4.94365841e-01,\n",
       "        -2.89637542e+00, -1.22627926e+00, -1.78170955e+00,\n",
       "        -1.83793223e+00, -1.61289275e+00,  1.02182972e+00,\n",
       "         1.56183529e+00,  1.96137464e+00, -3.49043250e-01,\n",
       "         1.52520800e+00],\n",
       "       [-1.44701648e+00,  1.51503551e+00,  8.75564873e-01,\n",
       "         1.58242500e+00,  1.25664043e+00, -8.12931716e-01,\n",
       "         4.59726620e+00,  5.24586737e-01,  1.60854781e+00,\n",
       "         3.69168472e+00, -8.26111257e-01,  1.15606415e+00,\n",
       "         1.06309843e+00, -1.01428048e-03,  1.12436450e+00,\n",
       "        -4.30764389e+00, -1.88352096e+00, -1.86531591e+00,\n",
       "        -2.72221658e-02, -1.72358632e+00,  1.24061882e+00,\n",
       "        -9.94410038e-01,  4.59891170e-01,  7.08988547e-01,\n",
       "         2.49599385e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix = np.array(train_matrix)\n",
    "train_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = train_matrix[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(twits):\n",
    "    \"\"\"\n",
    "    Does the padding as many times as required\n",
    "    \"\"\"\n",
    "    l = len(twits)\n",
    "    d= twits.shape[1]\n",
    "    padding = np.zeros(d)\n",
    "    \n",
    "    for i in range(0,max_length-l):\n",
    "        twits = np.append(twits,padding)\n",
    "        \n",
    "    return twits.reshape(max_length,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_matrix(t_matrix):\n",
    "    \"\"\"\n",
    "    Loops through all the vectorized tweets and calls the pad function on each of them.\n",
    "    \"\"\"\n",
    "    \n",
    "    padded_matrix =[]\n",
    "    for i,vectorized_tweet in enumerate(t_matrix):\n",
    "         padded_matrix.append(pad(vectorized_tweet))\n",
    "    padded_matrix = np.array(padded_matrix)\n",
    "    \n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_matrix = pad_matrix(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 54, 25)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df1['labels']\n",
    "labels_vector = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    labels_vector.append([label, 1 - label])\n",
    "labels_vector = np.array(labels_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "(128, 784)\n",
      "(128, 10)\n",
      "(5000, 54, 25)\n",
      "For iter  10\n",
      "Accuracy  0.8828125\n",
      "Loss  0.28635377\n",
      "__________________\n",
      "For iter  20\n",
      "Accuracy  0.84375\n",
      "Loss  0.4182338\n",
      "__________________\n",
      "For iter  30\n",
      "Accuracy  0.8359375\n",
      "Loss  0.37730753\n",
      "__________________\n",
      "For iter  40\n",
      "Accuracy  0.9453125\n",
      "Loss  0.16962945\n",
      "__________________\n",
      "For iter  50\n",
      "Accuracy  0.8671875\n",
      "Loss  0.36817774\n",
      "__________________\n",
      "For iter  60\n",
      "Accuracy  0.8984375\n",
      "Loss  0.24344419\n",
      "__________________\n",
      "For iter  70\n",
      "Accuracy  0.890625\n",
      "Loss  0.30431268\n",
      "__________________\n",
      "For iter  80\n",
      "Accuracy  0.8828125\n",
      "Loss  0.2812508\n",
      "__________________\n",
      "For iter  90\n",
      "Accuracy  0.921875\n",
      "Loss  0.21496773\n",
      "__________________\n",
      "For iter  100\n",
      "Accuracy  0.9375\n",
      "Loss  0.2061697\n",
      "__________________\n",
      "For iter  110\n",
      "Accuracy  0.890625\n",
      "Loss  0.27327046\n",
      "__________________\n",
      "For iter  120\n",
      "Accuracy  0.9375\n",
      "Loss  0.14902481\n",
      "__________________\n",
      "For iter  130\n",
      "Accuracy  0.8671875\n",
      "Loss  0.23568398\n",
      "__________________\n",
      "For iter  140\n",
      "Accuracy  0.9296875\n",
      "Loss  0.20599057\n",
      "__________________\n",
      "For iter  150\n",
      "Accuracy  0.9375\n",
      "Loss  0.15409315\n",
      "__________________\n",
      "For iter  160\n",
      "Accuracy  0.8984375\n",
      "Loss  0.24586205\n",
      "__________________\n",
      "For iter  170\n",
      "Accuracy  0.96875\n",
      "Loss  0.1215131\n",
      "__________________\n",
      "For iter  180\n",
      "Accuracy  0.953125\n",
      "Loss  0.1457867\n",
      "__________________\n",
      "For iter  190\n",
      "Accuracy  0.9453125\n",
      "Loss  0.13800438\n",
      "__________________\n",
      "For iter  200\n",
      "Accuracy  0.921875\n",
      "Loss  0.18618762\n",
      "__________________\n",
      "For iter  210\n",
      "Accuracy  0.921875\n",
      "Loss  0.15449211\n",
      "__________________\n",
      "For iter  220\n",
      "Accuracy  0.9140625\n",
      "Loss  0.20262529\n",
      "__________________\n",
      "For iter  230\n",
      "Accuracy  0.9375\n",
      "Loss  0.18963467\n",
      "__________________\n",
      "For iter  240\n",
      "Accuracy  0.8828125\n",
      "Loss  0.22194803\n",
      "__________________\n",
      "For iter  250\n",
      "Accuracy  0.9296875\n",
      "Loss  0.18347287\n",
      "__________________\n",
      "For iter  260\n",
      "Accuracy  0.953125\n",
      "Loss  0.15570699\n",
      "__________________\n",
      "For iter  270\n",
      "Accuracy  0.9609375\n",
      "Loss  0.118801035\n",
      "__________________\n",
      "For iter  280\n",
      "Accuracy  0.9453125\n",
      "Loss  0.1579933\n",
      "__________________\n",
      "For iter  290\n",
      "Accuracy  0.90625\n",
      "Loss  0.16951546\n",
      "__________________\n",
      "For iter  300\n",
      "Accuracy  0.9609375\n",
      "Loss  0.14558047\n",
      "__________________\n",
      "For iter  310\n",
      "Accuracy  0.90625\n",
      "Loss  0.20953725\n",
      "__________________\n",
      "For iter  320\n",
      "Accuracy  0.96875\n",
      "Loss  0.08099836\n",
      "__________________\n",
      "For iter  330\n",
      "Accuracy  0.9296875\n",
      "Loss  0.16208333\n",
      "__________________\n",
      "For iter  340\n",
      "Accuracy  0.9375\n",
      "Loss  0.18263742\n",
      "__________________\n",
      "For iter  350\n",
      "Accuracy  0.890625\n",
      "Loss  0.24113506\n",
      "__________________\n",
      "For iter  360\n",
      "Accuracy  0.9375\n",
      "Loss  0.15219867\n",
      "__________________\n",
      "For iter  370\n",
      "Accuracy  0.9453125\n",
      "Loss  0.12405661\n",
      "__________________\n",
      "For iter  380\n",
      "Accuracy  0.984375\n",
      "Loss  0.12243475\n",
      "__________________\n",
      "For iter  390\n",
      "Accuracy  0.921875\n",
      "Loss  0.12767313\n",
      "__________________\n",
      "For iter  400\n",
      "Accuracy  0.921875\n",
      "Loss  0.18560308\n",
      "__________________\n",
      "For iter  410\n",
      "Accuracy  0.953125\n",
      "Loss  0.16025776\n",
      "__________________\n",
      "For iter  420\n",
      "Accuracy  0.953125\n",
      "Loss  0.13366252\n",
      "__________________\n",
      "For iter  430\n",
      "Accuracy  0.953125\n",
      "Loss  0.238811\n",
      "__________________\n",
      "For iter  440\n",
      "Accuracy  0.953125\n",
      "Loss  0.16009808\n",
      "__________________\n",
      "For iter  450\n",
      "Accuracy  0.953125\n",
      "Loss  0.15414405\n",
      "__________________\n",
      "For iter  460\n",
      "Accuracy  0.953125\n",
      "Loss  0.12331687\n",
      "__________________\n",
      "For iter  470\n",
      "Accuracy  0.9296875\n",
      "Loss  0.2268481\n",
      "__________________\n",
      "For iter  480\n",
      "Accuracy  0.921875\n",
      "Loss  0.1720241\n",
      "__________________\n",
      "For iter  490\n",
      "Accuracy  0.9609375\n",
      "Loss  0.11400427\n",
      "__________________\n",
      "For iter  500\n",
      "Accuracy  0.9453125\n",
      "Loss  0.18778758\n",
      "__________________\n",
      "For iter  510\n",
      "Accuracy  0.9296875\n",
      "Loss  0.14779374\n",
      "__________________\n",
      "For iter  520\n",
      "Accuracy  0.96875\n",
      "Loss  0.1077478\n",
      "__________________\n",
      "For iter  530\n",
      "Accuracy  0.96875\n",
      "Loss  0.07036659\n",
      "__________________\n",
      "For iter  540\n",
      "Accuracy  0.9375\n",
      "Loss  0.12145796\n",
      "__________________\n",
      "For iter  550\n",
      "Accuracy  0.9453125\n",
      "Loss  0.16395581\n",
      "__________________\n",
      "For iter  560\n",
      "Accuracy  0.953125\n",
      "Loss  0.09764978\n",
      "__________________\n",
      "For iter  570\n",
      "Accuracy  0.96875\n",
      "Loss  0.07984094\n",
      "__________________\n",
      "For iter  580\n",
      "Accuracy  0.96875\n",
      "Loss  0.12423342\n",
      "__________________\n",
      "For iter  590\n",
      "Accuracy  0.9765625\n",
      "Loss  0.10424489\n",
      "__________________\n",
      "For iter  600\n",
      "Accuracy  0.9375\n",
      "Loss  0.12540011\n",
      "__________________\n",
      "For iter  610\n",
      "Accuracy  0.96875\n",
      "Loss  0.059339162\n",
      "__________________\n",
      "For iter  620\n",
      "Accuracy  0.9375\n",
      "Loss  0.16780366\n",
      "__________________\n",
      "For iter  630\n",
      "Accuracy  0.9453125\n",
      "Loss  0.16311209\n",
      "__________________\n",
      "For iter  640\n",
      "Accuracy  0.96875\n",
      "Loss  0.116987616\n",
      "__________________\n",
      "For iter  650\n",
      "Accuracy  0.9453125\n",
      "Loss  0.15550584\n",
      "__________________\n",
      "For iter  660\n",
      "Accuracy  0.9375\n",
      "Loss  0.13842282\n",
      "__________________\n",
      "For iter  670\n",
      "Accuracy  0.9296875\n",
      "Loss  0.19374153\n",
      "__________________\n",
      "For iter  680\n",
      "Accuracy  0.9453125\n",
      "Loss  0.14037639\n",
      "__________________\n",
      "For iter  690\n",
      "Accuracy  0.890625\n",
      "Loss  0.21244442\n",
      "__________________\n",
      "For iter  700\n",
      "Accuracy  0.921875\n",
      "Loss  0.2226859\n",
      "__________________\n",
      "For iter  710\n",
      "Accuracy  0.96875\n",
      "Loss  0.10055255\n",
      "__________________\n",
      "For iter  720\n",
      "Accuracy  0.9453125\n",
      "Loss  0.10172357\n",
      "__________________\n",
      "For iter  730\n",
      "Accuracy  0.9453125\n",
      "Loss  0.14046052\n",
      "__________________\n",
      "For iter  740\n",
      "Accuracy  0.984375\n",
      "Loss  0.06456234\n",
      "__________________\n",
      "For iter  750\n",
      "Accuracy  0.96875\n",
      "Loss  0.063853435\n",
      "__________________\n",
      "For iter  760\n",
      "Accuracy  0.921875\n",
      "Loss  0.16153921\n",
      "__________________\n",
      "For iter  770\n",
      "Accuracy  0.9765625\n",
      "Loss  0.09130222\n",
      "__________________\n",
      "For iter  780\n",
      "Accuracy  0.9765625\n",
      "Loss  0.076757714\n",
      "__________________\n",
      "For iter  790\n",
      "Accuracy  0.9765625\n",
      "Loss  0.08438895\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "#define constants\n",
    "#unrolled through 28 time steps\n",
    "time_steps=54\n",
    "#hidden LSTM units\n",
    "num_units=128\n",
    "#rows of 28 pixels\n",
    "n_input=25\n",
    "#learning rate for adam\n",
    "learning_rate=0.001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "n_classes=2\n",
    "#size of batch\n",
    "batch_size=128\n",
    "\n",
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "\n",
    "#defining the network\n",
    "lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n",
    "\n",
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    while iter<800:\n",
    "        batch_x,batch_y=next_batch(batch_size, padded_train_matrix, labels_vector)\n",
    "        sess.run(opt, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if iter %10==0:\n",
    "            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})\n",
    "            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
