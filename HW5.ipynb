{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMPS242 Homework 5 - Classifying Donald Trump and Hillary Clinton's Tweets\n",
    "#### Team Members:\n",
    "Olexiy Burov                             \n",
    "Karthik Balakrishna           \n",
    "Roy Shadmon     \n",
    "Sasidharan Mahalingam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required packages\n",
    "First import the rerquired packages required to create a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sasidharan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\n",
    "import gensim as gs\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "\n",
    "#Clearing any previous graph if present\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train data\n",
    "Import the tweets from the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11444\n",
      "Printing a sample of the train dataset tweets... \n",
      "\n",
      "\n",
      "['In Tampa, Florida- thank you to all of our outstanding volunteers who want to #MakeAmericaGreatAgain! <url>', 'Poll: @realDonaldTrump vs. @HillaryClinton among white Evangelicals. <url>', 'Obama on whether Trump could be trusted with US nuclear weapons: \"Make your own judgment\" <url>', '\"Hillary Clinton has never quit on anything in her life.\" —@FLOTUS #DemsInPhilly <url>', 'I LOVE NEW YORK! #NewYorkValues \\n<url>']\n",
      "Printing a sample of the test dataset tweets... \n",
      "\n",
      "\n",
      "['“Donald Trump—The Disrupter” will air on @FoxNews Saturday night and Sunday night at 8 PM ET. Anchored by @BretBaier. @johnrobertsFox', 'Nobody beats me on National Security. \\n<url>', 'Looks like I was right about NATO. I had no doubt. <url>', 'Jennifer is a terrific person. <url>', '\"@R_U_OK_UK: @realDonaldTrump @glozee1 @PaulManafort @CNN @DanScavino Vote trump to save the west. Don\\'t become like Europe - #WakeUpAmerica']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Have to implement the text manupilation to remove the https and other unwanted characters and strings\n",
    "\n",
    "#Loading the train dataset\n",
    "#df1 = pd.read_csv('HillaryTrumpTrainWithTags.csv')\n",
    "df1 = pd.read_csv('train.csv')\n",
    "#Loading the test dataset\n",
    "#df2 = pd.read_csv('TrumpHillaryTestWithTags.csv')\n",
    "df2 = pd.read_csv('test.csv')\n",
    "#Getting only the tweets from the train dataset\n",
    "all_tweets = df1['tweet'].str.replace(\"https://.*\", \"<url>\").tolist()\n",
    "#all_tweets = all_tweets + c_tweets\n",
    "print(len(all_tweets))\n",
    "#Doing a 70 - 30 split to form the train and test dataset\n",
    "train_tweets = all_tweets[0:1000]\n",
    "test_tweets = all_tweets[10000:11444]\n",
    "#Printing a sample of the train dataset tweets\n",
    "print('Printing a sample of the train dataset tweets... \\n\\n')\n",
    "print(train_tweets[0:5])\n",
    "#Printing a sample of the test dataset tweets\n",
    "print('Printing a sample of the test dataset tweets... \\n\\n')\n",
    "print(test_tweets[0:5])\n",
    "#Concatenating the train and test dataset tweets to form the word embeddings\n",
    "test_tweets2 = df2['tweet'].str.replace(\"https://.*\", \"<url>\").tolist()\n",
    "tweets = all_tweets + test_tweets2\n",
    "#print(df1['tweet'].str.replace(\"https://*\", \"<url>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the tweets\n",
    "Tokenize the tweets using keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few word index mappings... \n",
      "\n",
      "\n",
      "{'luxury': 3485, \"muted—there's\": 6923, '“looks': 5771, 'compañeros': 8543, 'tapes': 8276, '51': 4294, 'que': 391, 'selfishly': 6620, 'sweat': 2361, 'out': 63}\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the tweets\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "#Printing some of the word indexs\n",
    "sample_mapping = {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]} \n",
    "print('Printing a few word index mappings... \\n\\n')\n",
    "print(sample_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading GloVe word embeddings\n",
    "Load the downloaded pre-trained GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10186, 5)\n",
      "<class 'numpy.ndarray'>\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "#Setting the dimension of the word vectors\n",
    "\n",
    "#Note: I have used 200 dimensional vectors (I know it might overfit as the given data is very less). \n",
    "#Let's zero down on the optimal value after we get this to work\n",
    "\n",
    "embedding_dimension = 5\n",
    "embed_method = \"glove\"\n",
    "#Getting the word index representation of the tokenized words\n",
    "word_index = tokenizer.word_index\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "#print(index_word)\n",
    "\n",
    "\n",
    "#Create an empty dictionary that stores the word to index mappings\n",
    "if(embed_method == \"gensim\"):\n",
    "    embeddings_index = {}\n",
    "    #Import the GloVe word vectors\n",
    "    glove_data = 'glove.twitter.27B.200d.txt'\n",
    "    #Load the word vectors into memory\n",
    "    f = open(glove_data)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        value = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = value\n",
    "    f.close()\n",
    "\n",
    "    #Printing the number of word vectors loaded\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension), dtype=np.float64)\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "    \n",
    "else:\n",
    "    tokens = tokenizer.texts_to_sequences(tweets)\n",
    "    sentences = []\n",
    "    for sentence in tokens:\n",
    "        s = []\n",
    "        for index in sentence:\n",
    "            word = index_word[index]\n",
    "            s.append(word)\n",
    "        sentences.append(s)\n",
    "    #print(sentences)\n",
    "    model = gs.models.Word2Vec(sentences , min_count=1, \n",
    "                                          size = embedding_dimension, iter = 5000)\n",
    "    #model = gs.models.KeyedVectors.load_word2vec_format('glove.27B.200d.w2vformat.txt', binary=False)\n",
    "    #model.train(sentences , min_count=1, size = embedding_dimension, epochs = 1000)\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab) + 1, embedding_dimension),dtype=np.float32)\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "#Printing the shape of the word embedding matrix\n",
    "print(embedding_matrix.shape)\n",
    "print(type(embedding_matrix))\n",
    "zero_rows = np.where(~embedding_matrix.any(axis=1))[0]\n",
    "print(zero_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tweets from the training dataset and find word embedding matrix\n",
    "Get the training tweet dataset and convert that from strings into the word embedding matrix, the required input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#reshaping the training data to word to index sequences\n",
    "train_X = tokenizer.texts_to_sequences(train_tweets)\n",
    "#pad the shorter sentences to the length of the largest sentence\n",
    "train_X = tf.keras.preprocessing.sequence.pad_sequences(train_X, maxlen=32)\n",
    "#print the shape of the matrix of word vectors\n",
    "print(train_X.shape)\n",
    "print(type(train_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tweets from the test dataset and find the word embedding matrix for it\n",
    "Get the test tweets and find the word embedding for the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1444, 32)\n",
      "<class 'numpy.ndarray'>\n",
      "(1444, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#reshaping the test data to word to index sequences\n",
    "test_X = tokenizer.texts_to_sequences(test_tweets)\n",
    "#pad the shorter sentences to the length of the largest sentence\n",
    "test_X = tf.keras.preprocessing.sequence.pad_sequences(test_X, maxlen=32)\n",
    "#print the shape of the matrix of word vectors\n",
    "print(test_X.shape)\n",
    "print(type(test_X))\n",
    "\n",
    "#reshaping the test data to word to index sequences\n",
    "test_X2 = tokenizer.texts_to_sequences(test_tweets2)\n",
    "#pad the shorter sentences to the length of the largest sentence\n",
    "test_X2 = tf.keras.preprocessing.sequence.pad_sequences(test_X2, maxlen=32)\n",
    "#print the shape of the matrix of word vectors\n",
    "print(test_X2.shape)\n",
    "print(type(test_X2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define different parameters for the LSTM\n",
    "Now we define the number of time steps, number of hidden layers and the number of nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of time steps\n",
    "time_steps = 32\n",
    "#Batch size\n",
    "batch_size = 100\n",
    "#Number of hidden layers\n",
    "num_units = 5\n",
    "#Learning rate\n",
    "learning_rate = 0.001\n",
    "#Number of classes\n",
    "n_classes = 2\n",
    "#Size of the word vectors\n",
    "n_input = embedding_dimension\n",
    "#Regularizer parameter\n",
    "lambda_l2 = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for train and test data set\n",
    "Next, we form the train and test labels from the loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n",
      "(1444, 2)\n"
     ]
    }
   ],
   "source": [
    "#Creating an empty list to store the labels\n",
    "l1 = []\n",
    "#Get the train labels\n",
    "_ = df1['handle'].apply(lambda x: l1.append([1,0]) if x == 'realDonaldTrump' else l1.append([0,1]))\n",
    "#Convert the train label list into an numpy array\n",
    "all_labels = np.array(l1, dtype=np.float32)\n",
    "train_labels = all_labels[0:10000]\n",
    "print(train_labels.shape)\n",
    "#Empty the list to store the test labels\n",
    "#l1 = []\n",
    "#Populate the list with test labels\n",
    "#_ = df2['handle'].apply(lambda x: l1.append([1,0]) if x == 'realDonaldTrump' else l1.append([0,1]))\n",
    "#Convert the test label list into an numpy array\n",
    "#test_labels = np.array(l1, dtype=np.float64)\n",
    "test_labels = all_labels[10000:11444]\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM cell with Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_initializer(scale=1.0):\n",
    "    def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "        if partition_info is not None:\n",
    "            ValueError(\n",
    "                \"Do not know what to do with partition_info in BN_LSTMCell\")\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        q = q.reshape(shape)\n",
    "        return tf.constant(scale * q[:shape[0], :shape[1]], dtype=dtype)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def batch_norm(inputs, name_scope, is_training, epsilon=1e-3, decay=0.99):\n",
    "    with tf.variable_scope(name_scope):\n",
    "        size = inputs.get_shape().as_list()[1]\n",
    "\n",
    "        scale = tf.get_variable(\n",
    "            'scale', [size], initializer=tf.constant_initializer(0.1))\n",
    "        offset = tf.get_variable('offset', [size])\n",
    "\n",
    "        population_mean = tf.get_variable(\n",
    "            'population_mean', [size],\n",
    "            initializer=tf.zeros_initializer(), trainable=False)\n",
    "        population_var = tf.get_variable(\n",
    "            'population_var', [size],\n",
    "            initializer=tf.ones_initializer(), trainable=False)\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "\n",
    "        # The following part is based on the implementation of :\n",
    "        # https://github.com/cooijmanstim/recurrent-batch-normalization\n",
    "        train_mean_op = tf.assign(\n",
    "            population_mean,\n",
    "            population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var_op = tf.assign(\n",
    "            population_var, population_var * decay + batch_var * (1 - decay))\n",
    "\n",
    "        if is_training is True:\n",
    "            with tf.control_dependencies([train_mean_op, train_var_op]):\n",
    "                return tf.nn.batch_normalization(\n",
    "                    inputs, batch_mean, batch_var, offset, scale, epsilon)\n",
    "        else:\n",
    "            return tf.nn.batch_normalization(\n",
    "                inputs, population_mean, population_var, offset, scale,\n",
    "                epsilon)\n",
    "\n",
    "\n",
    "\n",
    "class BN_LSTMCell(RNNCell):\n",
    "    \"\"\"LSTM cell with Recurrent Batch Normalization.\n",
    "    This implementation is based on:\n",
    "         http://arxiv.org/abs/1603.09025\n",
    "    This implementation is also based on:\n",
    "         https://github.com/OlavHN/bnlstm\n",
    "         https://github.com/nicolas-ivanov/Seq2Seq_Upgrade_TensorFlow\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, is_training,\n",
    "                 use_peepholes=False, cell_clip=None,\n",
    "                 initializer=orthogonal_initializer(),\n",
    "                 num_proj=None, proj_clip=None,\n",
    "                 forget_bias=1.0,\n",
    "                 state_is_tuple=True,\n",
    "                 activation=tf.tanh):\n",
    "        \"\"\"Initialize the parameters for an LSTM cell.\n",
    "        Args:\n",
    "          num_units: int, The number of units in the LSTM cell.\n",
    "          is_training: bool, set True when training.\n",
    "          use_peepholes: bool, set True to enable diagonal/peephole\n",
    "            connections.\n",
    "          cell_clip: (optional) A float value, if provided the cell state\n",
    "            is clipped by this value prior to the cell output activation.\n",
    "          initializer: (optional) The initializer to use for the weight\n",
    "            matrices.\n",
    "          num_proj: (optional) int, The output dimensionality for\n",
    "            the projection matrices.  If None, no projection is performed.\n",
    "          forget_bias: Biases of the forget gate are initialized by default\n",
    "            to 1 in order to reduce the scale of forgetting at the beginning of\n",
    "            the training.\n",
    "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "            the `c_state` and `m_state`.  If False, they are concatenated\n",
    "            along the column axis.\n",
    "          activation: Activation function of the inner states.\n",
    "        \"\"\"\n",
    "        if not state_is_tuple:\n",
    "            tf.logging.log_first_n(\n",
    "                tf.logging.WARN,\n",
    "                \"%s: Using a concatenated state is slower and \"\n",
    "                \" will soon be deprecated.  Use state_is_tuple=True.\", 1, self)\n",
    "\n",
    "        self.num_units = num_units\n",
    "        self.is_training = is_training\n",
    "        self.use_peepholes = use_peepholes\n",
    "        self.cell_clip = cell_clip\n",
    "        self.num_proj = num_proj\n",
    "        self.proj_clip = proj_clip\n",
    "        self.initializer = initializer\n",
    "        self.forget_bias = forget_bias\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self.activation = activation\n",
    "\n",
    "        if num_proj:\n",
    "            self._state_size = (\n",
    "                LSTMStateTuple(num_units, num_proj)\n",
    "                if state_is_tuple else num_units + num_proj)\n",
    "            self._output_size = num_proj\n",
    "        else:\n",
    "            self._state_size = (\n",
    "                LSTMStateTuple(num_units, num_units)\n",
    "                if state_is_tuple else 2 * num_units)\n",
    "            self._output_size = num_units\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        num_proj = self.num_units if self.num_proj is None else self.num_proj\n",
    "\n",
    "        if self._state_is_tuple:\n",
    "            (c_prev, h_prev) = state\n",
    "        else:\n",
    "            c_prev = tf.slice(state, [0, 0], [-1, self.num_units])\n",
    "            h_prev = tf.slice(state, [0, self.num_units], [-1, num_proj])\n",
    "\n",
    "        dtype = inputs.dtype\n",
    "        input_size = inputs.get_shape().with_rank(2)[1]\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            if input_size.value is None:\n",
    "                raise ValueError(\n",
    "                    \"Could not infer input size from inputs.get_shape()[-1]\")\n",
    "\n",
    "            W_xh = tf.get_variable(\n",
    "                'W_xh',\n",
    "                [input_size, 4 * self.num_units],\n",
    "                initializer=self.initializer)\n",
    "            W_hh = tf.get_variable(\n",
    "                'W_hh',\n",
    "                [num_proj, 4 * self.num_units],\n",
    "                initializer=self.initializer)\n",
    "            bias = tf.get_variable('B', [4 * self.num_units])\n",
    "\n",
    "            xh = tf.matmul(inputs, W_xh)\n",
    "            hh = tf.matmul(h_prev, W_hh)\n",
    "\n",
    "            bn_xh = batch_norm(xh, 'xh', self.is_training)\n",
    "            bn_hh = batch_norm(hh, 'hh', self.is_training)\n",
    "\n",
    "            # i:input gate, j:new input, f:forget gate, o:output gate\n",
    "            lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)\n",
    "            i, j, f, o = tf.split(\n",
    "                value=lstm_matrix, num_or_size_splits=4, axis=1)\n",
    "\n",
    "            # Diagonal connections\n",
    "            if self.use_peepholes:\n",
    "                w_f_diag = tf.get_variable(\n",
    "                    \"W_F_diag\", shape=[self.num_units], dtype=dtype)\n",
    "                w_i_diag = tf.get_variable(\n",
    "                    \"W_I_diag\", shape=[self.num_units], dtype=dtype)\n",
    "                w_o_diag = tf.get_variable(\n",
    "                    \"W_O_diag\", shape=[self.num_units], dtype=dtype)\n",
    "\n",
    "            if self.use_peepholes:\n",
    "                c = c_prev * tf.sigmoid(f + self.forget_bias +\n",
    "                                        w_f_diag * c_prev) + \\\n",
    "                    tf.sigmoid(i + w_i_diag * c_prev) * self.activation(j)\n",
    "            else:\n",
    "                c = c_prev * tf.sigmoid(f + self.forget_bias) + \\\n",
    "                    tf.sigmoid(i) * self.activation(j)\n",
    "\n",
    "            if self.cell_clip is not None:\n",
    "                c = tf.clip_by_value(c, -self.cell_clip, self.cell_clip)\n",
    "\n",
    "            bn_c = batch_norm(c, 'cell', self.is_training)\n",
    "\n",
    "            if self.use_peepholes:\n",
    "                h = tf.sigmoid(o + w_o_diag * c) * self.activation(bn_c)\n",
    "            else:\n",
    "                h = tf.sigmoid(o) * self.activation(bn_c)\n",
    "\n",
    "            if self.num_proj is not None:\n",
    "                w_proj = tf.get_variable(\n",
    "                    \"W_P\", [self.num_units, num_proj], dtype=dtype)\n",
    "\n",
    "                h = tf.matmul(h, w_proj)\n",
    "                if self.proj_clip is not None:\n",
    "                    h = tf.clip_by_value(h, -self.proj_clip, self.proj_clip)\n",
    "\n",
    "            new_state = (LSTMStateTuple(c, h)\n",
    "                         if self._state_is_tuple else tf.concat(1, [c, h]))\n",
    "\n",
    "        return h, new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring the weeights and biases for the final fed forward network and placeholders for the input data\n",
    "Declaring variables for the weights and biases for the final feed forward network and the placeholders for the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a variable to hold the weights of the final feed forward layer\n",
    "out_weights = tf.Variable(tf.random_normal([num_units, n_classes], dtype=tf.float32), dtype=tf.float32)\n",
    "#Creating a variable to hold the biases of the final feed forward layer\n",
    "out_bias = tf.Variable(tf.random_normal([n_classes],dtype=tf.float32),dtype=tf.float32)\n",
    "\n",
    "#Input data placeholder\n",
    "#x = tf.placeholder(\"float64\",[None,time_steps,n_input])\n",
    "x = tf.Variable(tf.zeros([train_X.shape[0],time_steps,n_input], dtype=tf.float32),dtype=tf.float32)\n",
    "#x = tf.identity(embed)\n",
    "#Input label placeholder\n",
    "y = tf.placeholder(\"float32\",[None,n_classes])\n",
    "#y = tf.identity(train_labels)\n",
    "X = tf.placeholder(\"int32\", [None,32])\n",
    "word_vecs = tf.placeholder(\"float32\",[embedding_matrix.shape[0], n_input])\n",
    "train_flag = tf.placeholder(tf.bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the LSTM model\n",
    "Then, define and construct the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.reset_default_graph()\n",
    "#Copying values into placeholders\n",
    "x = tf.nn.embedding_lookup(word_vecs, X)\n",
    "\n",
    "#Reshaping the input dataset\n",
    "input_data = tf.unstack(x, time_steps, 1)\n",
    "\n",
    "#defining the network\n",
    "#lstm_layer= tf.contrib.rnn.LSTMCell(num_units,forget_bias=1)\n",
    "lstm_layer = BN_LSTMCell(num_units=num_units, is_training=train_flag, forget_bias=1, use_peepholes=True)\n",
    "lstm_layer = tf.nn.rnn_cell.DropoutWrapper(lstm_layer, output_keep_prob=0.5)\n",
    "#lstm_layer = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units, dropout_keep_prob=0.5) \n",
    "outputs,_= tf.contrib.rnn.static_rnn(lstm_layer,input_data,dtype=\"float32\")\n",
    "\n",
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
    "#defining regularization loss\n",
    "#reg_loss = lambda_l2 * sum( tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "#loss += reg_loss\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#test prediction\n",
    "test = tf.nn.softmax(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the LSTM model\n",
    "Now that we have set up the LSTM model, we run the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  1000\n",
      "Accuracy  0.62\n",
      "Loss  0.641231\n",
      "__________________\n",
      "For iter  2000\n",
      "Accuracy  0.7\n",
      "Loss  0.579099\n",
      "__________________\n",
      "For iter  3000\n",
      "Accuracy  0.77\n",
      "Loss  0.562348\n",
      "__________________\n",
      "For iter  4000\n",
      "Accuracy  0.76\n",
      "Loss  0.533054\n",
      "__________________\n",
      "For iter  5000\n",
      "Accuracy  0.75\n",
      "Loss  0.536147\n",
      "__________________\n",
      "For iter  6000\n",
      "Accuracy  0.82\n",
      "Loss  0.500232\n",
      "__________________\n",
      "For iter  7000\n",
      "Accuracy  0.74\n",
      "Loss  0.527343\n",
      "__________________\n",
      "For iter  8000\n",
      "Accuracy  0.73\n",
      "Loss  0.538795\n",
      "__________________\n",
      "For iter  9000\n",
      "Accuracy  0.79\n",
      "Loss  0.423148\n",
      "__________________\n",
      "Testing Accuracy: 0.722992\n",
      "[[ 0.11456414  0.88543588]\n",
      " [ 0.27194732  0.72805262]\n",
      " [ 0.86403149  0.13596851]\n",
      " [ 0.29079717  0.70920277]\n",
      " [ 0.33592841  0.66407156]\n",
      " [ 0.5874179   0.4125821 ]\n",
      " [ 0.6783812   0.32161885]\n",
      " [ 0.50130862  0.49869132]\n",
      " [ 0.54860324  0.45139676]\n",
      " [ 0.02242926  0.97757077]]\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(in_data, in_labels, no_of_samples):\n",
    "    indices = np.random.choice(in_data.shape[0],no_of_samples,replace=False)\n",
    "    return(in_data[indices], in_labels[indices])\n",
    "\n",
    "\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    while iter<10000:\n",
    "        batch_x, batch_y = generate_samples(train_X, train_labels, batch_size)\n",
    "        #print(batch_x.shape)\n",
    "        #batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "\n",
    "        sess.run(opt, feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y})\n",
    "        \n",
    "        if iter %1000 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y})\n",
    "            los = sess.run(loss,feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y, train_flag: True})\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "    output = sess.run(test, feed_dict={word_vecs:embedding_matrix, X: test_X2, y: test_labels, train_flag: False})\n",
    "    np.savetxt(\"prediction.csv\", output, delimiter=\",\")\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={word_vecs:embedding_matrix, X: test_X, y: test_labels,\n",
    "                                                            train_flag: False}))\n",
    "    print(output[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
