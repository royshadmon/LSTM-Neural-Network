{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMPS242 Homework 5 - Classifying Donald Trump and Hillary Clinton's Tweets\n",
    "#### Team Members:\n",
    "Alex                       \n",
    "Karthik Balakrishna           \n",
    "Roy Shadmon \n",
    "Sasidharan Mahalingam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required packages\n",
    "First import the rerquired packages required to create a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train data\n",
    "Import the tweets from the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a sample of the train dataset tweets... \n",
      "\n",
      "\n",
      "['In Tampa, Florida- thank you to all of our outstanding volunteers who want to #MakeAmericaGreatAgain! https://t.co/04qqpGylT7', 'Poll: @realDonaldTrump vs. @HillaryClinton among white Evangelicals. https://t.co/6ohwIh1Q24', 'Obama on whether Trump could be trusted with US nuclear weapons: \"Make your own judgment\" https://t.co/6OZtrfIwim https://t.co/Nj20PaXF2o', '\"Hillary Clinton has never quit on anything in her life.\" —@FLOTUS #DemsInPhilly https://t.co/mbg5H8QZA3', 'I LOVE NEW YORK! #NewYorkValues \\nhttps://t.co/dbTDhYAX1v']\n",
      "Printing a sample of the test dataset tweets... \n",
      "\n",
      "\n",
      "['Live from Charlotte: @POTUS hits the trail with Hillary for the first time in this campaign. Watch: https://t.co/1IJMGhsqEc', 'The worst part is, Trump\\'s disturbing policies for immigrant families go way beyond just \"building a wall.\" https://t.co/szrTF9eB0E', 'You can watch 360 video live from the podium! https://t.co/yqcIsBUdAi #RNCinCLE #TrumpIsWithYou #MakeAmericaGreatAgain', 'No wonder Donald Trump is hiding his tax returns. #debatenight https://t.co/gcvsadMwHJ', '\"Let us be vigilant, but not afraid…we choose resolve, not fear. We will not turn on each other or undermine each other.” —Hillary']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Have to implement the text manupilation to remove the https and other unwanted characters and strings\n",
    "\n",
    "#Loading the train dataset\n",
    "df1 = pd.read_csv('train.csv')\n",
    "#Loading the test dataset\n",
    "df2 = pd.read_csv('test.csv')\n",
    "#Getting only the tweets from the train dataset\n",
    "train_tweets = df1['tweet'].tolist()\n",
    "#Getting only the tweets from the test dataset\n",
    "test_tweets = df2['tweet'].tolist()\n",
    "#Printing a sample of the train dataset tweets\n",
    "print('Printing a sample of the train dataset tweets... \\n\\n')\n",
    "print(train_tweets[0:5])\n",
    "#Printing a sample of the test dataset tweets\n",
    "print('Printing a sample of the test dataset tweets... \\n\\n')\n",
    "print(test_tweets[0:5])\n",
    "#Concatenating the train and test dataset tweets to form the word embeddings\n",
    "tweets = train_tweets + test_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the tweets\n",
    "Tokenize the tweets using keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few word index mappings... \n",
      "\n",
      "\n",
      "{'beau': 12252, 'streep': 9770, 'luxurious': 9595, \"kurteichenwald's\": 11192, '6rh849dlyz': 13901, 'djkdazt3wv': 11083, 'quo': 3621, 'introduction': 3687, 'equality': 897, 'realericjallen': 12351}\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the tweets\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "#Printing some of the word indexs\n",
    "sample_mapping = {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]} \n",
    "print('Printing a few word index mappings... \\n\\n')\n",
    "print(sample_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading GloVe word embeddings\n",
    "Load the downloaded pre-trained GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Create an empty dictionary that stores the word to index mappings\n",
    "embeddings_index = {}\n",
    "#Import the GloVe word vectors\n",
    "glove_data = 'glove.twitter.27B.200d.txt'\n",
    "#Load the word vectors into memory\n",
    "f = open(glove_data)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    "\n",
    "#Printing the number of word vectors loaded\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the words to the GloVe word vectors\n",
    "Map the tokenized words to 200 dimensional word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14120, 200)\n"
     ]
    }
   ],
   "source": [
    "#Setting the dimension of the word vectors\n",
    "\n",
    "#Note: I have used 200 dimensional vectors (I know it might overfit as the given data is very less). \n",
    "#Let's zero down on the optimal value after we get this to work\n",
    "\n",
    "embedding_dimension = 200\n",
    "#Getting the word index representation of the tokenized words\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "#Printing the shape of the word embedding matrix\n",
    "print(embedding_matrix.shape)\n",
    " \n",
    "#TODO:    \n",
    "#One future improvement that we could do is to train these embedding using the tweets instead of setting the words not present to zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tweets from the training dataset and for it's one hot vector representation\n",
    "Get the training tweet dataset and convert that from strings into a one-hot vector representation, so that we can use the word embeddings to create the required input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 ..., 0 0 0]\n",
      " [0 1 1 ..., 0 0 0]\n",
      " [0 1 1 ..., 0 0 0]\n",
      " ..., \n",
      " [0 1 1 ..., 0 0 0]\n",
      " [0 1 1 ..., 1 0 0]\n",
      " [0 1 1 ..., 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Create a tokenizer to tokenize the train tweets\n",
    "train_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "#Fit the tokenizer on the train dataset\n",
    "train_tokenizer.fit_on_texts(train_tweets)\n",
    "#Creating the one-hot vector representation of the train tweets\n",
    "train_one_hot = np.array(train_tokenizer.texts_to_matrix(train_tweets)).astype(int)\n",
    "print(train_one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to one-hot vectors into matrices using the word embeddings\n",
    "Now that we have the one-hot vector representation of the tweets, use the created GloVe word vectors to generate the input dataset that has the word vectors instead of the one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the input dataset is:\n",
      "(5000, 11939, 200)\n"
     ]
    }
   ],
   "source": [
    "#Now create the required input to our LSTM using the embedding and the one-hot vectors\n",
    "embed = tf.nn.embedding_lookup(embedding_matrix, train_one_hot)\n",
    "#Printing the dimensions of the input dataset\n",
    "print('The shape of the input dataset is:')\n",
    "print(embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
