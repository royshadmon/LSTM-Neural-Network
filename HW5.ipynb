{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMPS242 Homework 5 - Classifying Donald Trump and Hillary Clinton's Tweets\n",
    "#### Team Members:\n",
    "Alex                       \n",
    "Karthik Balakrishna           \n",
    "Roy Shadmon     \n",
    "Sasidharan Mahalingam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the required packages\n",
    "First import the rerquired packages required to create a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Clearing any previous graph if present\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Train data\n",
    "Import the tweets from the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a sample of the train dataset tweets... \n",
      "\n",
      "\n",
      "['In Tampa, Florida- thank you to all of our outstanding volunteers who want to #MakeAmericaGreatAgain! https://t.co/04qqpGylT7', 'Poll: @realDonaldTrump vs. @HillaryClinton among white Evangelicals. https://t.co/6ohwIh1Q24', 'Obama on whether Trump could be trusted with US nuclear weapons: \"Make your own judgment\" https://t.co/6OZtrfIwim https://t.co/Nj20PaXF2o', '\"Hillary Clinton has never quit on anything in her life.\" —@FLOTUS #DemsInPhilly https://t.co/mbg5H8QZA3', 'I LOVE NEW YORK! #NewYorkValues \\nhttps://t.co/dbTDhYAX1v']\n",
      "Printing a sample of the test dataset tweets... \n",
      "\n",
      "\n",
      "['Live from Charlotte: @POTUS hits the trail with Hillary for the first time in this campaign. Watch: https://t.co/1IJMGhsqEc', 'The worst part is, Trump\\'s disturbing policies for immigrant families go way beyond just \"building a wall.\" https://t.co/szrTF9eB0E', 'You can watch 360 video live from the podium! https://t.co/yqcIsBUdAi #RNCinCLE #TrumpIsWithYou #MakeAmericaGreatAgain', 'No wonder Donald Trump is hiding his tax returns. #debatenight https://t.co/gcvsadMwHJ', '\"Let us be vigilant, but not afraid…we choose resolve, not fear. We will not turn on each other or undermine each other.” —Hillary']\n"
     ]
    }
   ],
   "source": [
    "#TODO: Have to implement the text manupilation to remove the https and other unwanted characters and strings\n",
    "\n",
    "#Loading the train dataset\n",
    "df1 = pd.read_csv('train.csv')\n",
    "#Loading the test dataset\n",
    "df2 = pd.read_csv('test.csv')\n",
    "#Getting only the tweets from the train dataset\n",
    "train_tweets = df1['tweet'].tolist()\n",
    "#Getting only the tweets from the test dataset\n",
    "test_tweets = df2['tweet'].tolist()\n",
    "#Printing a sample of the train dataset tweets\n",
    "print('Printing a sample of the train dataset tweets... \\n\\n')\n",
    "print(train_tweets[0:5])\n",
    "#Printing a sample of the test dataset tweets\n",
    "print('Printing a sample of the test dataset tweets... \\n\\n')\n",
    "print(test_tweets[0:5])\n",
    "#Concatenating the train and test dataset tweets to form the word embeddings\n",
    "tweets = train_tweets + test_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing the tweets\n",
    "Tokenize the tweets using keras Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing a few word index mappings... \n",
      "\n",
      "\n",
      "{'rxdsrjwaal': 7085, 'knhqcetatv': 8161, '1cj7ofbqiz': 13320, 'buildthewall': 8512, 'marlondmarshall': 11608, 'reflect': 3314, 'a': 6, 'w9xq5fhhyv': 13143, 'years”': 12469, 'chriscjackson': 6544}\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the tweets\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "#Printing some of the word indexs\n",
    "sample_mapping = {k: tokenizer.word_index[k] for k in list(tokenizer.word_index)[:10]} \n",
    "print('Printing a few word index mappings... \\n\\n')\n",
    "print(sample_mapping)\n",
    "\n",
    "word_dict = tokenizer.fit_on_texts(tweets)\n",
    "print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading GloVe word embeddings\n",
    "Load the downloaded pre-trained GloVe word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Create an empty dictionary that stores the word to index mappings\n",
    "embeddings_index = {}\n",
    "#Import the GloVe word vectors\n",
    "glove_data = 'glove.twitter.27B.200d.txt'\n",
    "#Load the word vectors into memory\n",
    "f = open(glove_data)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    value = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = value\n",
    "f.close()\n",
    "\n",
    "#Printing the number of word vectors loaded\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping the words to the GloVe word vectors\n",
    "Map the tokenized words to 200 dimensional word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14120, 20)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Setting the dimension of the word vectors\n",
    "\n",
    "#Note: I have used 200 dimensional vectors (I know it might overfit as the given data is very less). \n",
    "#Let's zero down on the optimal value after we get this to work\n",
    "\n",
    "embedding_dimension = 20\n",
    "#Getting the word index representation of the tokenized words\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dimension), dtype=np.float64)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dimension]\n",
    "\n",
    "\n",
    "#Printing the shape of the word embedding matrix\n",
    "print(embedding_matrix.shape)\n",
    "print(type(embedding_matrix))\n",
    "#TODO:    \n",
    "#One future improvement that we could do is to train these embedding using the tweets instead of setting the words not present to zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tweets from the training dataset and find word embedding matrix\n",
    "Get the training tweet dataset and convert that from strings into the word embedding matrix, the required input to the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#reshaping the training data to word to index sequences\n",
    "train_X = tokenizer.texts_to_sequences(train_tweets)\n",
    "#pad the shorter sentences to the length of the largest sentence\n",
    "train_X = tf.keras.preprocessing.sequence.pad_sequences(train_X)\n",
    "#print the shape of the matrix of word vectors\n",
    "print(train_X.shape)\n",
    "print(type(train_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the tweets from the test dataset and find the word embedding matrix for it\n",
    "Get the test tweets and find the word embedding for the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1444, 32)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#reshaping the test data to word to index sequences\n",
    "test_X = tokenizer.texts_to_sequences(test_tweets)\n",
    "#pad the shorter sentences to the length of the largest sentence\n",
    "test_X = tf.keras.preprocessing.sequence.pad_sequences(test_X, maxlen=32)\n",
    "#print the shape of the matrix of word vectors\n",
    "print(test_X.shape)\n",
    "print(type(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define different parameters for the LSTM\n",
    "Now we define the number of time steps, number of hidden layers and the number of nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set number of time steps\n",
    "time_steps = 32\n",
    "#Batch size\n",
    "batch_size = 3000\n",
    "#Number of hidden layers\n",
    "num_units = 10\n",
    "#Learning rate\n",
    "learning_rate = 0.0001\n",
    "#Number of classes\n",
    "n_classes = 2\n",
    "#Size of the word vectors\n",
    "n_input = embedding_dimension\n",
    "#Regularizer parameter\n",
    "lambda_l1 = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating labels for train and test data set\n",
    "Next, we form the train and test labels from the loaded dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 2)\n",
      "(1444, 2)\n"
     ]
    }
   ],
   "source": [
    "#Creating an empty list to store the labels\n",
    "l1 = []\n",
    "#Get the train labels\n",
    "_ = df1['handle'].apply(lambda x: l1.append([1,0]) if x == 'realDonaldTrump' else l1.append([0,1]))\n",
    "#Convert the train label list into an numpy array\n",
    "train_labels = np.array(l1, dtype=np.float64)\n",
    "print(train_labels.shape)\n",
    "#Empty the list to store the test labels\n",
    "l1 = []\n",
    "#Populate the list with test labels\n",
    "_ = df2['handle'].apply(lambda x: l1.append([1,0]) if x == 'realDonaldTrump' else l1.append([0,1]))\n",
    "#Convert the test label list into an numpy array\n",
    "test_labels = np.array(l1, dtype=np.float64)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaring the weeights and biases for the final fed forward network and placeholders for the input data\n",
    "Declaring variables for the weights and biases for the final feed forward network and the placeholders for the input dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a variable to hold the weights of the final feed forward layer\n",
    "out_weights = tf.Variable(tf.random_normal([num_units, n_classes], dtype=tf.float64), dtype=tf.float64)\n",
    "#Creating a variable to hold the biases of the final feed forward layer\n",
    "out_bias = tf.Variable(tf.random_normal([n_classes],dtype=tf.float64),dtype=tf.float64)\n",
    "\n",
    "#Input data placeholder\n",
    "x = tf.placeholder(\"float64\",[None,time_steps,n_input])\n",
    "#x = tf.identity(embed)\n",
    "#Input label placeholder\n",
    "y = tf.placeholder(\"float64\",[None,n_classes])\n",
    "#y = tf.identity(train_labels)\n",
    "X = tf.placeholder(\"int64\", [None,32])\n",
    "word_vecs = tf.placeholder(\"float64\",[14120, n_input])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the LSTM model\n",
    "Then, define and construct the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Copying values into placeholders\n",
    "x = tf.nn.embedding_lookup(word_vecs, X)\n",
    "\n",
    "#Reshaping the input dataset\n",
    "input_data = tf.unstack(x, time_steps, 1)\n",
    "\n",
    "#defining the network\n",
    "lstm_layer= tf.contrib.rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "outputs,_= tf.contrib.rnn.static_rnn(lstm_layer,input_data,dtype=\"float64\")\n",
    "\n",
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))\n",
    "#defining regularization loss\n",
    "reg_loss = lambda_l1 * sum( tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "loss += reg_loss\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the LSTM model\n",
    "Now that we have set up the LSTM model, we run the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For iter  1000\n",
      "Accuracy  0.672333333333\n",
      "Loss  0.671868788936\n",
      "__________________\n",
      "For iter  2000\n",
      "Accuracy  0.761\n",
      "Loss  0.536462860962\n",
      "__________________\n",
      "For iter  3000\n",
      "Accuracy  0.799666666667\n",
      "Loss  0.473048544779\n",
      "__________________\n",
      "For iter  4000\n",
      "Accuracy  0.82\n",
      "Loss  0.437935748671\n",
      "__________________\n",
      "For iter  5000\n",
      "Accuracy  0.842333333333\n",
      "Loss  0.40363875234\n",
      "__________________\n",
      "For iter  6000\n",
      "Accuracy  0.857666666667\n",
      "Loss  0.366835579067\n",
      "__________________\n",
      "For iter  7000\n",
      "Accuracy  0.873333333333\n",
      "Loss  0.345415383395\n",
      "__________________\n",
      "For iter  8000\n",
      "Accuracy  0.876666666667\n",
      "Loss  0.336497112236\n",
      "__________________\n",
      "For iter  9000\n",
      "Accuracy  0.872666666667\n",
      "Loss  0.333074909513\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "def generate_samples(in_data, in_labels, no_of_samples):\n",
    "    indices = np.random.choice(in_data.shape[0],no_of_samples,replace=False)\n",
    "    return(in_data[indices], in_labels[indices])\n",
    "\n",
    "\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    while iter<10000:\n",
    "        batch_x, batch_y = generate_samples(train_X, train_labels, batch_size)\n",
    "        #print(batch_x.shape)\n",
    "        #batch_x=batch_x.reshape((batch_size,time_steps,n_input))\n",
    "\n",
    "        sess.run(opt, feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y})\n",
    "        \n",
    "        if iter %1000 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y})\n",
    "            los = sess.run(loss,feed_dict={word_vecs:embedding_matrix, X: batch_x, y: batch_y})\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "    output = sess.run(tf.argmax(prediction, 1), feed_dict={word_vecs:embedding_matrix, X: test_X, y: test_labels})\n",
    "    print(\"Testing Accuracy:\", sess.run(accuracy, feed_dict={word_vecs:embedding_matrix, X: test_X, y: test_labels}))\n",
    "    print(\"your prediction for X_test is :\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
