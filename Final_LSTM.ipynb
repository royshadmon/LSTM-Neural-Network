{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/sasidharan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndf2 = pd.read_table(\\'glove.twitter.27B.25d.txt\\', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\\n\\ndef vectorize(w):\\n  return df2.loc[w].as_matrix()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "import nltk \n",
    "from tensorflow.contrib.rnn import RNNCell, LSTMStateTuple\n",
    "import gensim as gs\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "df1 = pd.read_csv('HillaryTrumpTrainWithTags.csv')\n",
    "df2 = pd.read_csv('TrumpHillaryTestWithTags.csv')\n",
    "\"\"\"\n",
    "df2 = pd.read_table('glove.twitter.27B.25d.txt', sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "def vectorize(w):\n",
    "  return df2.loc[w].as_matrix()\n",
    "\"\"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleToNumber(handle):\n",
    "    if handle == \"realDonaldTrump\":\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tokenized_tweets'] = df1.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n",
    "df1['labels'] = df1.apply(lambda row: handleToNumber(row['handle']), axis=1)\n",
    "df2['tokenized_tweets'] = df2.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df1['tokenized_tweets']\n",
    "tweets_test = df2['tokenized_tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "_ = df1.apply(lambda row: length.append(len(row['tokenized_tweets'])), axis=1)\n",
    "max_length = max(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = gs.models.Word2Vec(df1['tokenized_tweets'].append(df2['tokenized_tweets']) , min_count=1, size = 20, iter = 2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = []\n",
    "test_matrix = []\n",
    "\n",
    "for i, tweet in enumerate(tweets):\n",
    "    #print(type(string))\n",
    "    train_matrix.append(vectorizer.wv[tweet])\n",
    "    \n",
    "for i, test_tweet in enumerate(tweets_test):\n",
    "    test_matrix.append(vectorizer.wv[test_tweet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.array(train_matrix).reshape(len(train_matrix),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.array(train_matrix)\n",
    "train_matrix[0]\n",
    "\n",
    "test_matrix = np.array(test_matrix)\n",
    "test_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = train_matrix[4999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(twits):\n",
    "    \"\"\"\n",
    "    Does the padding as many times as required\n",
    "    \"\"\"\n",
    "    l = len(twits)\n",
    "    d= twits.shape[1]\n",
    "    padding = np.zeros(d)\n",
    "    \n",
    "    for i in range(0,max_length-l):\n",
    "        twits = np.append(twits,padding)\n",
    "        \n",
    "    return twits.reshape(max_length,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_matrix(t_matrix):\n",
    "    \"\"\"\n",
    "    Loops through all the vectorized tweets and calls the pad function on each of them.\n",
    "    \"\"\"\n",
    "    \n",
    "    padded_matrix =[]\n",
    "    for i,vectorized_tweet in enumerate(t_matrix):\n",
    "         padded_matrix.append(pad(vectorized_tweet))\n",
    "    padded_matrix = np.array(padded_matrix)\n",
    "    \n",
    "    return padded_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train_matrix = pad_matrix(train_matrix)\n",
    "padded_test_matrix = pad_matrix(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "padded_train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df1['labels']\n",
    "labels_vector = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    labels_vector.append([label, 1 - label])\n",
    "labels_vector = np.array(labels_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonal_initializer(scale=1.0):\n",
    "    def _initializer(shape, dtype=tf.float32, partition_info=None):\n",
    "        if partition_info is not None:\n",
    "            ValueError(\n",
    "                \"Do not know what to do with partition_info in BN_LSTMCell\")\n",
    "        flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "        a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "        u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "        q = u if u.shape == flat_shape else v\n",
    "        q = q.reshape(shape)\n",
    "        return tf.constant(scale * q[:shape[0], :shape[1]], dtype=dtype)\n",
    "    return _initializer\n",
    "\n",
    "\n",
    "def batch_norm(inputs, name_scope, is_training, epsilon=1e-3, decay=0.99):\n",
    "    with tf.variable_scope(name_scope):\n",
    "        size = inputs.get_shape().as_list()[1]\n",
    "\n",
    "        scale = tf.get_variable(\n",
    "            'scale', [size], initializer=tf.constant_initializer(0.1))\n",
    "        offset = tf.get_variable('offset', [size])\n",
    "\n",
    "        population_mean = tf.get_variable(\n",
    "            'population_mean', [size],\n",
    "            initializer=tf.zeros_initializer(), trainable=False)\n",
    "        population_var = tf.get_variable(\n",
    "            'population_var', [size],\n",
    "            initializer=tf.ones_initializer(), trainable=False)\n",
    "        batch_mean, batch_var = tf.nn.moments(inputs, [0])\n",
    "\n",
    "        # The following part is based on the implementation of :\n",
    "        # https://github.com/cooijmanstim/recurrent-batch-normalization\n",
    "        train_mean_op = tf.assign(\n",
    "            population_mean,\n",
    "            population_mean * decay + batch_mean * (1 - decay))\n",
    "        train_var_op = tf.assign(\n",
    "            population_var, population_var * decay + batch_var * (1 - decay))\n",
    "\n",
    "        if is_training is True:\n",
    "            with tf.control_dependencies([train_mean_op, train_var_op]):\n",
    "                return tf.nn.batch_normalization(\n",
    "                    inputs, batch_mean, batch_var, offset, scale, epsilon)\n",
    "        else:\n",
    "            return tf.nn.batch_normalization(\n",
    "                inputs, population_mean, population_var, offset, scale,\n",
    "                epsilon)\n",
    "\n",
    "\n",
    "\n",
    "class BN_LSTMCell(RNNCell):\n",
    "    \"\"\"LSTM cell with Recurrent Batch Normalization.\n",
    "    This implementation is based on:\n",
    "         http://arxiv.org/abs/1603.09025\n",
    "    This implementation is also based on:\n",
    "         https://github.com/OlavHN/bnlstm\n",
    "         https://github.com/nicolas-ivanov/Seq2Seq_Upgrade_TensorFlow\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_units, is_training,\n",
    "                 use_peepholes=False, cell_clip=None,\n",
    "                 initializer=orthogonal_initializer(),\n",
    "                 num_proj=None, proj_clip=None,\n",
    "                 forget_bias=1.0,\n",
    "                 state_is_tuple=True,\n",
    "                 activation=tf.tanh):\n",
    "        \"\"\"Initialize the parameters for an LSTM cell.\n",
    "        Args:\n",
    "          num_units: int, The number of units in the LSTM cell.\n",
    "          is_training: bool, set True when training.\n",
    "          use_peepholes: bool, set True to enable diagonal/peephole\n",
    "            connections.\n",
    "          cell_clip: (optional) A float value, if provided the cell state\n",
    "            is clipped by this value prior to the cell output activation.\n",
    "          initializer: (optional) The initializer to use for the weight\n",
    "            matrices.\n",
    "          num_proj: (optional) int, The output dimensionality for\n",
    "            the projection matrices.  If None, no projection is performed.\n",
    "          forget_bias: Biases of the forget gate are initialized by default\n",
    "            to 1 in order to reduce the scale of forgetting at the beginning of\n",
    "            the training.\n",
    "          state_is_tuple: If True, accepted and returned states are 2-tuples of\n",
    "            the `c_state` and `m_state`.  If False, they are concatenated\n",
    "            along the column axis.\n",
    "          activation: Activation function of the inner states.\n",
    "        \"\"\"\n",
    "        if not state_is_tuple:\n",
    "            tf.logging.log_first_n(\n",
    "                tf.logging.WARN,\n",
    "                \"%s: Using a concatenated state is slower and \"\n",
    "                \" will soon be deprecated.  Use state_is_tuple=True.\", 1, self)\n",
    "\n",
    "        self.num_units = num_units\n",
    "        self.is_training = is_training\n",
    "        self.use_peepholes = use_peepholes\n",
    "        self.cell_clip = cell_clip\n",
    "        self.num_proj = num_proj\n",
    "        self.proj_clip = proj_clip\n",
    "        self.initializer = initializer\n",
    "        self.forget_bias = forget_bias\n",
    "        self._state_is_tuple = state_is_tuple\n",
    "        self.activation = activation\n",
    "\n",
    "        if num_proj:\n",
    "            self._state_size = (\n",
    "                LSTMStateTuple(num_units, num_proj)\n",
    "                if state_is_tuple else num_units + num_proj)\n",
    "            self._output_size = num_proj\n",
    "        else:\n",
    "            self._state_size = (\n",
    "                LSTMStateTuple(num_units, num_units)\n",
    "                if state_is_tuple else 2 * num_units)\n",
    "            self._output_size = num_units\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size\n",
    "\n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "\n",
    "        num_proj = self.num_units if self.num_proj is None else self.num_proj\n",
    "\n",
    "        if self._state_is_tuple:\n",
    "            (c_prev, h_prev) = state\n",
    "        else:\n",
    "            c_prev = tf.slice(state, [0, 0], [-1, self.num_units])\n",
    "            h_prev = tf.slice(state, [0, self.num_units], [-1, num_proj])\n",
    "\n",
    "        dtype = inputs.dtype\n",
    "        input_size = inputs.get_shape().with_rank(2)[1]\n",
    "\n",
    "        with tf.variable_scope(scope or type(self).__name__):\n",
    "            if input_size.value is None:\n",
    "                raise ValueError(\n",
    "                    \"Could not infer input size from inputs.get_shape()[-1]\")\n",
    "\n",
    "            W_xh = tf.get_variable(\n",
    "                'W_xh',\n",
    "                [input_size, 4 * self.num_units],\n",
    "                initializer=self.initializer)\n",
    "            W_hh = tf.get_variable(\n",
    "                'W_hh',\n",
    "                [num_proj, 4 * self.num_units],\n",
    "                initializer=self.initializer)\n",
    "            bias = tf.get_variable('B', [4 * self.num_units])\n",
    "\n",
    "            xh = tf.matmul(inputs, W_xh)\n",
    "            hh = tf.matmul(h_prev, W_hh)\n",
    "\n",
    "            bn_xh = batch_norm(xh, 'xh', self.is_training)\n",
    "            bn_hh = batch_norm(hh, 'hh', self.is_training)\n",
    "\n",
    "            # i:input gate, j:new input, f:forget gate, o:output gate\n",
    "            lstm_matrix = tf.nn.bias_add(tf.add(bn_xh, bn_hh), bias)\n",
    "            i, j, f, o = tf.split(\n",
    "                value=lstm_matrix, num_or_size_splits=4, axis=1)\n",
    "\n",
    "            # Diagonal connections\n",
    "            if self.use_peepholes:\n",
    "                w_f_diag = tf.get_variable(\n",
    "                    \"W_F_diag\", shape=[self.num_units], dtype=dtype)\n",
    "                w_i_diag = tf.get_variable(\n",
    "                    \"W_I_diag\", shape=[self.num_units], dtype=dtype)\n",
    "                w_o_diag = tf.get_variable(\n",
    "                    \"W_O_diag\", shape=[self.num_units], dtype=dtype)\n",
    "\n",
    "            if self.use_peepholes:\n",
    "                c = c_prev * tf.sigmoid(f + self.forget_bias +\n",
    "                                        w_f_diag * c_prev) + \\\n",
    "                    tf.sigmoid(i + w_i_diag * c_prev) * self.activation(j)\n",
    "            else:\n",
    "                c = c_prev * tf.sigmoid(f + self.forget_bias) + \\\n",
    "                    tf.sigmoid(i) * self.activation(j)\n",
    "\n",
    "            if self.cell_clip is not None:\n",
    "                c = tf.clip_by_value(c, -self.cell_clip, self.cell_clip)\n",
    "\n",
    "            bn_c = batch_norm(c, 'cell', self.is_training)\n",
    "\n",
    "            if self.use_peepholes:\n",
    "                h = tf.sigmoid(o + w_o_diag * c) * self.activation(bn_c)\n",
    "            else:\n",
    "                h = tf.sigmoid(o) * self.activation(bn_c)\n",
    "\n",
    "            if self.num_proj is not None:\n",
    "                w_proj = tf.get_variable(\n",
    "                    \"W_P\", [self.num_units, num_proj], dtype=dtype)\n",
    "\n",
    "                h = tf.matmul(h, w_proj)\n",
    "                if self.proj_clip is not None:\n",
    "                    h = tf.clip_by_value(h, -self.proj_clip, self.proj_clip)\n",
    "\n",
    "            new_state = (LSTMStateTuple(c, h)\n",
    "                         if self._state_is_tuple else tf.concat(1, [c, h]))\n",
    "\n",
    "        return h, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#define constants\n",
    "#unrolled through 28 time steps\n",
    "time_steps=54\n",
    "#hidden LSTM units\n",
    "num_units=25\n",
    "#rows of 28 pixels\n",
    "n_input=20\n",
    "#learning rate for adam\n",
    "learning_rate=0.0001\n",
    "#mnist is meant to be classified in 10 classes(0-9).\n",
    "n_classes=2\n",
    "#size of batch\n",
    "batch_size=100\n",
    "\n",
    "#weights and biases of appropriate shape to accomplish above task\n",
    "out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))\n",
    "out_bias=tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "#defining placeholders\n",
    "#input image placeholder\n",
    "start_x=tf.placeholder(\"float\",[None,time_steps,n_input])\n",
    "x=tf.Variable(tf.zeros([batch_size,time_steps,n_input], dtype=tf.float32),dtype=tf.float32)\n",
    "x = tf.identity(start_x)\n",
    "\n",
    "#input label placeholder\n",
    "y=tf.placeholder(\"float\",[None,n_classes])\n",
    "train_flag = tf.placeholder(tf.bool)\n",
    "\n",
    "#processing the input tensor from [batch_size,n_steps,n_input] to \"time_steps\" number of [batch_size,n_input] tensors\n",
    "input=tf.unstack(x ,time_steps,1)\n",
    "\n",
    "#defining the network\n",
    "#lstm_layer=rnn.BasicLSTMCell(num_units,forget_bias=1)\n",
    "lstm_layer = BN_LSTMCell(num_units=num_units, is_training=train_flag, forget_bias=1, use_peepholes=True)\n",
    "lstm_layer = tf.nn.rnn_cell.DropoutWrapper(lstm_layer, output_keep_prob=0.5)\n",
    "outputs,_=rnn.static_rnn(lstm_layer,input,dtype=\"float32\")\n",
    "\n",
    "#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication\n",
    "prediction=tf.matmul(outputs[-1],out_weights)+out_bias\n",
    "test=tf.nn.softmax(prediction)\n",
    "\n",
    "#loss_function\n",
    "loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))\n",
    "#optimization\n",
    "opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "#model evaluation\n",
    "correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "#initialize variables\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    iter=1\n",
    "    while iter<8000:\n",
    "        batch_x,batch_y=next_batch(batch_size, padded_train_matrix, labels_vector)\n",
    "        sess.run(opt, feed_dict={start_x: batch_x, y: batch_y, train_flag:True})\n",
    "\n",
    "        if iter %1000==0:\n",
    "            acc=sess.run(accuracy,feed_dict={start_x:batch_x,y:batch_y,train_flag:True})\n",
    "            los=sess.run(loss,feed_dict={start_x:batch_x,y:batch_y,train_flag:True})\n",
    "            pred=sess.run(test, feed_dict={start_x:batch_x,y:batch_y,train_flag:False})\n",
    "#             print(pred)\n",
    "# #             print(batch_y)\n",
    "            print(\"For iter \",iter)\n",
    "            print(\"Accuracy \",acc)\n",
    "            print(\"Loss \",los)\n",
    "            print(\"__________________\")\n",
    "\n",
    "        iter=iter+1\n",
    "    pred=sess.run(test, feed_dict={start_x:padded_test_matrix, train_flag:False})\n",
    "    result = np.array(pred, dtype=np.float)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape\n",
    "a = np.zeros(shape=(1444,3))\n",
    "for i, res in enumerate(result):\n",
    "    a[i] = [i, result[i][1], result[i][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)\n",
    "np.set_printoptions(suppress=True)\n",
    "np.savetxt('test.out', a, delimiter=',', fmt='%1.19f')   # X is an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
